import os
import re
import cv2
import glob
import json
import time
import random
import string
import imageio
import collections
import numpy as np
from PIL import Image
from typing import Tuple
from shapely.geometry import Polygon
from abc import ABC, abstractmethod
import cv2
import numpy as np
import torch.utils.data as data
import torch
from io import BytesIO
import base64

from synthetic_dataset_tools import SyntheticDataset
from augment import (
    add_noise,
    box_augment,
    flip_lr,
    flip_pair,
    mask2bbox,
)

def convert_action_to_int(actions):
	""" Converts a string of an action to an integer
	"""
	action_ints = []
	for action in actions:
		if action == "take":
			action_ints.append(1)
		elif action == "put":
			action_ints.append(2)
		elif action == "shift":
			action_ints.append(3)
		else:
			print("Action ", action, " not recognized!")
			raise Exception
	return action_ints

class BaseDataset(data.Dataset, ABC):
    """This class is an abstract base class (ABC) for datasets.
    """

    def __init__(self, configuration):
        """Initialize the class; save the configuration in the class.
        """
        self.configuration = configuration

    @abstractmethod
    def __len__(self):
        """Return the total number of images in the dataset."""
        return 0

    @abstractmethod
    def __getitem__(self, index):
        """Return a data point (usually data and labels in
            a supervised setting).
        """
        pass

    def pre_epoch_callback(self, epoch):
        """Callback to be called before every epoch.
        """
        pass

    def post_epoch_callback(self, epoch):
        """Callback to be called after every epoch.
        """
        pass

class SyntheticPairsDataSet(BaseDataset):
    """Dataset of synthetic image pairs and change masks from synthetic images generated by blender
    Using official splits from imagesets dir.
    """

    DELIM_CHAR = ["-", "_"]
    IMG_FILE1 = "0.png"  # _1.png
    IMG_FILE2 = "1.png"  # _2.png
    LABEL_FILE = "_label.png"
    BBOX_FILE = "_bbox.json"

    def __init__(self, configuration):
        self.root = configuration["root"]
        self.split = configuration["mode"]
        assert self.split in ["train", "train_small", "val", "test"]
        self.files = collections.defaultdict(list)
        self.img_normalize = configuration["normalize"]
        self.crops = configuration["crop"]
        self.resize = configuration["resize"]
        self.spatial_resolution = configuration["spatial_resolution"]

        if not os.path.exists('/root/.imageio/freeimage'):
            imageio.plugins.freeimage.download()

        # For overfitting always use first 4 samples
        if configuration["overfit"]:
            print("Overfitting to 4 samples")
            with open(os.path.join('imagesets/train.txt'), 'r') as imgsetFile:
                scenes = imgsetFile.readlines()
            scenes = [e[:-1]for e in scenes]
            scenes.sort()
            scenes = scenes[:4]
        else:
            with open(os.path.join('imagesets', self.split+'.txt'), 'r') as imgsetFile:
                scenes = imgsetFile.readlines()
            scenes = [e[:-1]for e in scenes]

        # Use downloaded scenes only
        downloaded_files = [file for file in os.listdir(self.root)]
        scenes = [scene for scene in scenes if any(scene in file for file in downloaded_files)]

        anno_data = SyntheticDataset('synthetic_anno.json', self.root)
        image_ids = anno_data.getImgIds(scenes)
        ann_ids = anno_data.getAnnIds(image_ids)

        info = anno_data.mapScenesToInfo(scenes)
        for k,v in info.items():
            self.files[self.split].append(
                {
                    "img1": os.path.join(self.root, v["img1"]),
                    "img2": os.path.join(self.root, v["img2"]),
                    "label": v["label"],
                    "actions": convert_action_to_int(v["actions"]),
                    "bbox": v["bbox"],
                    "depth1": os.path.join(self.root, v["depth1"]),
                    "depth2": os.path.join(self.root, v["depth2"])
                }
            )

        self.mean = torch.tensor([0.406 * 255, 0.456 * 255, 0.485 * 255]).view(3, 1, 1)
        self.std = torch.tensor([0.225 * 255, 0.224 * 255, 0.229 * 255]).view(3, 1, 1)
        self.mean_depth = torch.tensor(7.7304)
        self.depth_std = torch.tensor(1.4214)

        self.res_x = self.spatial_resolution[0]
        self.res_y = self.spatial_resolution[1]

        self.aug = configuration["augment"]

        self.augment_params = {
            "box_augment_params": {
                "max_boxes": 3,
                "min_height_mult": 0.1,
                "max_height_mult": 0.5,
                "min_width_mult": 0.1,
                "max_width_mult": 0.3,
                "sat_prob": 0.5,
                "sat_min": 0.5,
                "sat_max": 1.5,
                "brightness_prob": 0.5,
                "brightness_min": 0.5,
                "brightness_max": 1.5,
            },
            "noise_params": {
                "qe_low": 0.65,
                "qe_high": 0.72,
                "bit_depth": 8,
                "baseline": 0,
                "sensitivity_low": 1.2,
                "sensitivity_high": 1.7,
                "dark_noise_low": 2.5,
                "dark_noise_high": 3.5,
            },
            "flip_pair": 0.0,
            "flip_lr": 0.0,
        }


    def __len__(self):
        return len(self.files[self.split])

    def augment(
        self, img1, img2, label, bboxes, depth1, depth2
    ):
        img1, img2 = add_noise(img1, img2, self.augment_params["noise_params"])
        img1, img2 = box_augment(img1, img2, self.augment_params["box_augment_params"])
        if random.random() < self.augment_params["flip_pair"]:
            img1, img2, bboxes, depth1, depth2 = flip_pair(img1, img2, bboxes, depth1, depth2)
        if random.random() < self.augment_params["flip_lr"]:
            img1, img2, label, bboxes, depth1, depth2 = flip_lr(img1, img2, label, bboxes, depth1, depth2)
        return img1, img2, label, bboxes, depth1, depth2

    def resize_imgs(
        self, img1, img2, label, depth1, depth2
    ):
        img1 = cv2.resize(img1, (self.res_x, self.res_y))
        img2 = cv2.resize(img2, (self.res_x, self.res_y))
        depth1 = cv2.resize(depth1, (self.res_x, self.res_y))
        depth2 = cv2.resize(depth2, (self.res_x, self.res_y))
        label = cv2.resize(label, (self.res_x, self.res_y), interpolation=cv2.INTER_NEAREST)
        return img1, img2, label, depth1, depth2

    def crop(
        self, img1, img2, label, bboxes, depth1, depth2
    ):
        height, width = img1.shape[:2]

        bbox = mask2bbox(label)
        if bbox is not None:
            change_rmin, change_cmin, change_rmax, change_cmax = bbox
        else:
            change_rmin, change_cmin, change_rmax, change_cmax = 0, 0, height, width

        bbheight = change_rmax - change_rmin
        bbwidth = change_cmax - change_cmin

        if bbheight < self.res_y:
            rmin = max(change_rmin - self.res_y + bbheight, 0)
            rmax = change_rmin
        else:
            rmin = change_rmin
            rmax = change_rmin - self.res_y + bbheight
        if bbwidth < self.res_x:
            cmin = max(change_cmin - self.res_x + bbwidth, 0)
            cmax = change_cmin
        else:
            cmin = change_cmin
            cmax = change_cmin - self.res_x + bbwidth

        x = random.randint(cmin, cmax)
        y = random.randint(rmin, rmax)
        x = min(x, width - self.res_x - 1)
        y = min(y, height - self.res_y - 1)
        ymax = y + self.res_y
        xmax = x + self.res_x
        img1 = img1[y:ymax, x:xmax, :]
        img2 = img2[y:ymax, x:xmax, :]
        label = label[y:ymax, x:xmax]
        depth1 = depth1[y:ymax, x:xmax]
        depth2 = depth2[y:ymax, x:xmax]
        # TODO: Modify bbox cropping to account for point order
        '''
        bboxes[:, :4] *= np.array([height, width, height, width])
        ya = np.fmax(bboxes[:, 0], y)
        xa = np.fmax(bboxes[:, 1], x)
        yb = np.fmin(bboxes[:, 2], ymax)
        xb = np.fmin(bboxes[:, 3], xmax)

        inds = np.logical_and((xb - xa) > 0, (yb - ya) > 0)
        xa, xb, ya, yb = xa[inds], xb[inds], ya[inds], yb[inds]
        bboxes = bboxes[inds, :]
        bboxes[:, 0] = ya - y
        bboxes[:, 1] = xa - x
        bboxes[:, 2] = yb - y
        bboxes[:, 3] = xb - x
        bboxes[:, :4] /= np.array([self.res_y, self.res_x, self.res_y, self.res_x])
        '''
        return img1, img2, label, bboxes, depth1, depth2

    def get_store(self, index: int):
        store = os.path.basename(self.files[self.split][index]["img1"]).split("_")[0]
        return store

    def __getitem__(self, index: int):
        datafiles = self.files[self.split][index]

        # Images
        img_file1, img_file2 = datafiles["img1"], datafiles["img2"]
        
        img1, img2 = (
            cv2.imread(img_file1),
            cv2.imread(img_file2),
        )
        height, width, _ = img1.shape
        
        # Label
        label_shifts, label_takes, label_puts = np.zeros((height, width)), np.zeros((height, width)), np.zeros((height, width))
        for i in range(len(datafiles["label"])):
            segmentation_points = datafiles["label"][i]
            action = datafiles["actions"][i]
            if action == 1:
                for contour in segmentation_points:
                    # Switching x,y coords in segmentation points
                    formatted_points = []
                    for j in range(0, len(contour), 2):
                        formatted_points.append((contour[j+1], contour[j]))
                    polygon = Polygon(formatted_points)
                    int_coords = lambda x: np.array(x).round().astype(int)
                    exterior = [int_coords(polygon.exterior.coords)]
                    label_takes = cv2.fillPoly(label_takes, exterior, action)
            elif action == 2:
                for contour in segmentation_points:
                    # Switching x,y coords in segmentation points
                    formatted_points = []
                    for j in range(0, len(contour), 2):
                        formatted_points.append((contour[j+1], contour[j]))
                    polygon = Polygon(formatted_points)
                    int_coords = lambda x: np.array(x).round().astype(int)
                    exterior = [int_coords(polygon.exterior.coords)]
                    label_puts = cv2.fillPoly(label_puts, exterior, action)
            elif action == 3:
                for contour in segmentation_points:
                    # Switching x,y coords in segmentation points
                    formatted_points = []
                    for j in range(0, len(contour), 2):
                        formatted_points.append((contour[j+1], contour[j]))
                    polygon = Polygon(formatted_points)
                    int_coords = lambda x: np.array(x).round().astype(int)
                    exterior = [int_coords(polygon.exterior.coords)]
                    label_shifts = cv2.fillPoly(label_shifts, exterior, action)
            else:
                pass
        # Give priority to takes, then puts, then shifts
        label = np.where(label_takes != 0, label_takes, label_puts)
        label = np.where(label != 0, label, label_shifts)

        # TODO: Bounding Boxes
        bboxes = datafiles["bbox"]
        bboxes = np.array(bboxes)
        
        # Depth
        depth1, depth2 = (
            imageio.imread(datafiles["depth1"], format="EXR-FI"),
            imageio.imread(datafiles["depth2"], format="EXR-FI")
        )

        if self.crops and not self.resize:
            img1, img2, label, bboxes, depth1, depth2 = self.crop(img1, img2, label, bboxes, depth1, depth2)
        elif self.resize and not self.crops:
            img1, img2, label, depth1, depth2 = self.resize_imgs(img1, img2, label, depth1, depth2)
        elif self.crops and self.resize:
            if random.random() > 0.5:
                img1, img2, label, depth1, depth2 = self.resize_imgs(img1, img2, label, depth1, depth2)
            else:
                img1, img2, label, bboxes, depth1, depth2 = self.crop(img1, img2, label, bboxes, depth1, depth2)
        if self.aug:
            img1, img2, label, bboxes, depth1, depth2 = self.augment(img1, img2, label, bboxes, depth1, depth2)

        # Debug: save images and labels
        scene = '_'.join(datafiles["img1"].split('/')[-1].split('_')[:3])
        cv2.imwrite('debug/{}_img1.jpg'.format(scene), img1)
        cv2.imwrite('debug/{}_img2.jpg'.format(scene), img2)
        cv2.imwrite('debug/{}_label.jpg'.format(scene), label)
        #imageio.show_formats()
        #imageio.imwrite('debug/{}_depth1.exr'.format(tmp_name), depth1.astype("float32"), format="EXR-FI")
        #imageio.imwrite('debug/{}_depth2.exr'.format(tmp_name), depth2.astype("float32"), format="EXR-FI")
        
        img1 = torch.from_numpy(img1).permute(2, 0, 1).contiguous()
        img2 = torch.from_numpy(img2).permute(2, 0, 1).contiguous()
        label = torch.from_numpy(label)
        depth1 = torch.from_numpy(depth1).contiguous()
        depth2 = torch.from_numpy(depth2).contiguous()
        depth1 = torch.unsqueeze(depth1, dim=0)
        depth2 = torch.unsqueeze(depth2, dim=0)

        '''
        bboxes[:, 0] = (bboxes[:, 2] + bboxes[:, 0]) / 2
        bboxes[:, 1] = (bboxes[:, 3] + bboxes[:, 1]) / 2
        bboxes[:, 2] = (bboxes[:, 2] - bboxes[:, 0]) * 2
        bboxes[:, 3] = (bboxes[:, 3] - bboxes[:, 1]) * 2
        bboxes = bboxes[0:4, :4]
        bboxes[:, 0:2] -= 0.5
        '''
        bboxes = torch.from_numpy(bboxes)
        

        if self.img_normalize:
            img1 = (img1 - self.mean) / self.std
            img2 = (img2 - self.mean) / self.std
            depth1 = (depth1 - self.mean_depth) / self.depth_std
            depth2 = (depth2 - self.mean_depth) / self.depth_std

        return img1, img2, label, '_'.join(datafiles["img1"].split('/')[-1].split('_')[:3]), depth1, depth2

    @staticmethod
    def collate_fn(batch):
        img1, img2, label, scene, depth1, depth2 = zip(*batch)
        return (
            torch.stack(img1, 0),
            torch.stack(img2, 0),
            torch.stack(label, 0),
            #torch.nn.utils.rnn.pad_sequence(bboxes, batch_first=True, padding_value=-1.0),
            scene,
            torch.stack(depth1, 0),
            torch.stack(depth2, 0)
        )

# Load the dataset json
class StandardSimDataset():
    def __init__(self, annotation_path, image_dir):
        self.annotation_path = annotation_path
        self.image_dir = image_dir
        self.colors = dict({'take': (255, 0, 0), 'put': (0, 255, 0), 'shift': (0, 0, 255)})
        
        json_file = open(self.annotation_path)
        self.coco = json.load(json_file)
        json_file.close()
        
        self.process_info()
        self.process_licenses()
        self.process_categories()
        self.process_images()
        self.process_segmentations()
        
        
    def display_info(self):
        print('Dataset Info:')
        print('=============')
        for key, item in self.info.items():
            print('  {}: {}'.format(key, item))
        
        requirements = [['description', str],
                        ['url', str],
                        ['version', str],
                        ['year', int],
                        ['contributor', str],
                        ['date_created', str]]
        for req, req_type in requirements:
            if req not in self.info:
                print('ERROR: {} is missing'.format(req))
            elif type(self.info[req]) != req_type:
                print('ERROR: {} should be type {}'.format(req, str(req_type)))
        print('')

        
    def display_licenses(self):
        print('Licenses:')
        print('=========')
        
        requirements = [['id', int],
                        ['url', str],
                        ['name', str]]
        for license in self.licenses:
            for key, item in license.items():
                print('  {}: {}'.format(key, item))
            for req, req_type in requirements:
                if req not in license:
                    print('ERROR: {} is missing'.format(req))
                elif type(license[req]) != req_type:
                    print('ERROR: {} should be type {}'.format(req, str(req_type)))
            print('')
        print('')
        
    def display_categories(self):
        print('Categories:')
        print('=========')
        for sc_key, sc_val in self.super_categories.items():
            print('  super_category: {}'.format(sc_key))
            for cat_id in sc_val:
                print('    id {}: {}'.format(cat_id, self.categories[cat_id]['name']))
            print('')
    
    def display_image(self, image_id, show_polys=True, show_bbox=True, actions=['take','put','shift']):
        print('Image:')
        print('======')
        if image_id == 'random':
            image_id = random.choice(list(self.images.keys()))
        
        # Print the image info
        image = self.images[image_id]
        for key, val in image.items():
            print('  {}: {}'.format(key, val))
            
        # Open the image
        image_path = os.path.join(self.image_dir, image['image2'])
        image_pil = Image.open(image_path)

        buffer = BytesIO()
        image_pil.save(buffer, format='PNG')
        buffer.seek(0)

        data_uri = base64.b64encode(buffer.read()).decode('ascii')
        image_path = "data:image/png;base64,{0}".format(data_uri)
            
        # Calculate the size and adjusted display size
        max_width = 1280
        image_width, image_height = image_pil.size
        adjusted_width = min(image_width, max_width)
        adjusted_ratio = adjusted_width / image_width
        adjusted_height = adjusted_ratio * image_height
        
        # Create list of polygons to be drawn
        polygons = {}
        bbox_polygons = {}
        rle_regions = {}
        poly_colors = {}
        all_formatted_points = []
        print('   segmentations:')
        image = cv2.imread(os.path.join(self.image_dir, image['image2']))
        # cv2 reads image in BGR, so convert it to RGB for visualization
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        overlay = image.copy()
        for i, segm in enumerate(self.segmentations[image_id]):
            if segm['action'] in actions:
                polygons_list = []
                # Correct the polygon segmentation point order
                for segmentation_points in segm['segmentation']:
                    segmentation_points = np.multiply(segmentation_points, adjusted_ratio).astype(int)
                    # Switching x,y coords in segmentation points
                    formatted_points = []
                    for i in range(0, len(segmentation_points), 2):
                        formatted_points.append((segmentation_points[i+1], segmentation_points[i]))
                    polygons_list.append(formatted_points)
                
                polygons[segm['id']] = polygons_list
                # Add all polygons to cv2 overlay
                if show_polys:
                    for poly in polygons_list:
                        polygon = Polygon(poly)
                        int_coords = lambda x: np.array(x).round().astype(np.int32)
                        exterior = [int_coords(polygon.exterior.coords)]
                        overlay = cv2.fillPoly(overlay, exterior, color=self.colors[segm['action']])#color=(255, 255, 0)), color=(5*i, 5*i, 0)
                        alpha=0.5
                        image = cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0, image)
                # Correct bounding box point order
                bbox = segm['bbox']
                bbox_points = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1],
                               bbox[0] + bbox[2], bbox[1] + bbox[3], bbox[0], bbox[1] + bbox[3],
                               bbox[0], bbox[1]]
                bbox_points = np.multiply(bbox_points, adjusted_ratio).astype(int)
                bbox_polygons[segm['id']] = str(bbox_points).lstrip('[').rstrip(']')
                # Add bounding box for this polygon
                if show_bbox:
                    cv2.rectangle(overlay, (int(bbox[1]), int(bbox[0])), (int(bbox[1]+bbox[3]), int(bbox[0]+bbox[2])), (255, 0, 0), 2)
                    cv2.putText(overlay, segm['action'], (int(bbox[1]), int(bbox[0])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)
                # Print details
                print('    {}:{}:{}'.format(segm['id'], segm['action'], self.categories[segm['category_id']]))
        
        fig = plt.figure(figsize=(8,6), dpi=200, facecolor='w', edgecolor='k')
        plt.imshow(overlay,)
        plt.axis('off')
        
        return None
        
    def process_info(self):
        self.info = self.coco['info']
    
    def process_licenses(self):
        self.licenses = self.coco['licenses']
    
    def process_categories(self):
        self.categories = {}
        self.super_categories = {}
        for category in self.coco['categories']:
            cat_id = category['id']
            
            # Add category to the categories dict
            if cat_id not in self.categories:
                self.categories[cat_id] = category
            else:
                print("ERROR: Skipping duplicate category id: {}".format(category))
                
    def process_images(self):
        self.images = {}
        for image in self.coco['images']:
            image_id = image['id']
            if image_id in self.images:
                print("ERROR: Skipping duplicate image id: {}".format(image))
            else:
                self.images[image_id] = image
                
    def process_segmentations(self):
        self.segmentations = {}
        for segmentation in self.coco['annotations']:
            image_id = segmentation['image_id']
            if image_id not in self.segmentations:
                self.segmentations[image_id] = []
            self.segmentations[image_id].append(segmentation)
